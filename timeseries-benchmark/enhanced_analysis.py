"""
Enhanced Analysis Module for Time-Series Benchmark
Adds critical missing analyses to strengthen your findings
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from scipy import stats
from sklearn.metrics import roc_auc_score

def load_results(run_dir):
    """Load your existing results"""
    run_dir = Path(run_dir)
    summary = pd.read_csv(run_dir / "exec_summary.csv")
    
    # Load all trace files
    traces = {}
    for trace_file in (run_dir / "traces").glob("*.csv"):
        key = trace_file.stem  # e.g., "nyc_taxi__ARIMA"
        traces[key] = pd.read_csv(trace_file)
    
    return summary, traces


def analysis_1_separability_vs_detection(summary, traces, output_dir):
    """
    CRITICAL ANALYSIS: Does residual separability predict detection success?
    This tests: Can models create separable residuals even if detector fails?
    """
    output_dir = Path(output_dir)
    output_dir.mkdir(exist_ok=True)
    
    print("\n" + "="*80)
    print("ANALYSIS 1: RESIDUAL SEPARABILITY vs DETECTION SUCCESS")
    print("="*80)
    
    # Calculate enhanced separability metrics for each trace
    enhanced_metrics = []
    
    for key, trace_df in traces.items():
        dataset_name, model_name = key.split("__")
        
        # Get test split residuals
        test_df = trace_df[trace_df['split'] == 'test'].copy()
        if len(test_df) == 0:
            continue
            
        residuals = test_df['residual'].values
        
        # We need labels - reconstruct from flags or load from original data
        # For now, use flag column (1 = detected anomaly)
        # In real run, load original labels
        has_flags = 'flag' in test_df.columns and test_df['flag'].notna().any()
        
        if not has_flags:
            continue
            
        flags = test_df['flag'].values
        
        # Calculate multiple separability metrics
        normal_res = np.abs(residuals[flags == 0])
        anomaly_res = np.abs(residuals[flags == 1])
        
        if len(anomaly_res) == 0:
            separation_ratio = 1.0
            effect_size = 0.0
            auc_residual = 0.5
        else:
            # Metric 1: Ratio of means
            separation_ratio = np.mean(anomaly_res) / (np.mean(normal_res) + 1e-8)
            
            # Metric 2: Cohen's d (effect size)
            pooled_std = np.sqrt((np.var(normal_res) + np.var(anomaly_res)) / 2)
            effect_size = (np.mean(anomaly_res) - np.mean(normal_res)) / (pooled_std + 1e-8)
            
            # Metric 3: AUC using residual magnitude as score
            labels_binary = flags.copy()
            try:
                auc_residual = roc_auc_score(labels_binary, np.abs(residuals))
            except:
                auc_residual = 0.5
        
        # Get F1 from summary
        f1 = summary[(summary['Dataset']==dataset_name) & 
                     (summary['Model']==model_name)]['F1'].values[0]
        
        smape = summary[(summary['Dataset']==dataset_name) & 
                       (summary['Model']==model_name)]['sMAPE'].values[0]
        
        enhanced_metrics.append({
            'Dataset': dataset_name,
            'Model': model_name,
            'separation_ratio': separation_ratio,
            'effect_size': effect_size,
            'auc_residual': auc_residual,
            'F1': f1,
            'sMAPE': smape,
            'n_normal': len(normal_res),
            'n_anomaly': len(anomaly_res)
        })
    
    sep_df = pd.DataFrame(enhanced_metrics)
    sep_df.to_csv(output_dir / "separability_analysis.csv", index=False)
    
    # VISUALIZATION 1: Separability vs F1
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    
    colors = {'ARIMA': 'blue', 'Toto': 'green', 'GraniteTTM': 'red'}
    
    for ax, metric, title in zip(axes, 
                                  ['separation_ratio', 'effect_size', 'auc_residual'],
                                  ['Separation Ratio', "Cohen's d", 'AUC of Residuals']):
        for model in sep_df['Model'].unique():
            df_m = sep_df[sep_df['Model'] == model]
            ax.scatter(df_m[metric], df_m['F1'], 
                      label=model, s=120, alpha=0.7, 
                      color=colors.get(model, 'gray'),
                      edgecolors='black')
            
            # Add dataset labels
            for _, row in df_m.iterrows():
                ax.annotate(row['Dataset'][:10], 
                           (row[metric], row['F1']),
                           fontsize=7, alpha=0.6,
                           xytext=(3, 3), textcoords='offset points')
        
        # Correlation
        corr = sep_df[[metric, 'F1']].corr().iloc[0, 1]
        ax.text(0.05, 0.95, f'r = {corr:.3f}', 
               transform=ax.transAxes, fontsize=11,
               verticalalignment='top',
               bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
        
        ax.set_xlabel(title, fontsize=11)
        ax.set_ylabel('F1 Score', fontsize=11)
        ax.grid(True, alpha=0.3, linestyle='--')
        ax.legend(fontsize=9)
    
    plt.suptitle('Residual Separability Metrics vs Detection Performance\n' +
                 'Higher separability should predict better detection if detector is adequate',
                 fontsize=13, y=1.02)
    plt.tight_layout()
    plt.savefig(output_dir / 'separability_vs_f1_comprehensive.png', 
                dpi=150, bbox_inches='tight')
    plt.close()
    
    # KEY INSIGHT EXTRACTION
    print("\nüìä SEPARABILITY CORRELATIONS:")
    for metric in ['separation_ratio', 'effect_size', 'auc_residual']:
        corr = sep_df[[metric, 'F1']].corr().iloc[0, 1]
        print(f"  {metric:20s} vs F1: r = {corr:+.3f}")
    
    # Compare to forecast quality correlation
    corr_forecast = sep_df[['sMAPE', 'F1']].corr().iloc[0, 1]
    corr_separability = sep_df[['auc_residual', 'F1']].corr().iloc[0, 1]
    
    print(f"\nüîç CRITICAL COMPARISON:")
    print(f"  Forecast Quality (sMAPE) vs F1:        r = {corr_forecast:+.3f}")
    print(f"  Residual Separability (AUC) vs F1:     r = {corr_separability:+.3f}")
    
    if abs(corr_separability) > abs(corr_forecast):
        print("\n  ‚ö†Ô∏è  SEPARABILITY predicts detection better than forecast quality!")
        print("      ‚Üí Detector is successfully using residual patterns")
        print("      ‚Üí Improving forecast quality will help via better residuals")
    else:
        print("\n  ‚ö†Ô∏è  FORECAST QUALITY predicts detection better than separability!")
        print("      ‚Üí Detector may have fundamental limitations")
        print("      ‚Üí Need better detection algorithms, not just better forecasts")
    
    return sep_df


def analysis_2_dataset_difficulty(summary, output_dir):
    """
    Quantify dataset difficulty and see if harder datasets benefit more 
    from better models
    """
    output_dir = Path(output_dir)
    
    print("\n" + "="*80)
    print("ANALYSIS 2: DATASET DIFFICULTY & MODEL SELECTION")
    print("="*80)
    
    # Calculate dataset difficulty (average F1 across all models)
    difficulty = summary.groupby('Dataset').agg({
        'F1': ['mean', 'std', 'min', 'max'],
        'sMAPE': 'mean'
    }).round(3)
    
    difficulty.columns = ['_'.join(col).strip() for col in difficulty.columns.values]
    difficulty = difficulty.sort_values('F1_mean', ascending=False)
    
    print("\nüìä DATASET DIFFICULTY RANKING (easier ‚Üí harder):")
    print(difficulty.to_string())
    
    difficulty.to_csv(output_dir / 'dataset_difficulty.csv')
    
    # VISUALIZATION: Does model choice matter more for hard datasets?
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # Plot 1: F1 range by dataset (shows which datasets have consensus)
    datasets = summary['Dataset'].unique()
    f1_ranges = []
    for ds in datasets:
        f1_vals = summary[summary['Dataset'] == ds]['F1'].values
        f1_ranges.append({
            'Dataset': ds,
            'mean': np.mean(f1_vals),
            'min': np.min(f1_vals),
            'max': np.max(f1_vals),
            'range': np.max(f1_vals) - np.min(f1_vals)
        })
    
    range_df = pd.DataFrame(f1_ranges).sort_values('mean', ascending=False)
    
    ax = axes[0]
    x = range(len(range_df))
    ax.barh(x, range_df['range'], alpha=0.6, color='skyblue', edgecolor='black')
    ax.set_yticks(x)
    ax.set_yticklabels(range_df['Dataset'])
    ax.set_xlabel('F1 Score Range (max - min across models)', fontsize=11)
    ax.set_title('Model Choice Impact by Dataset\nLarge range = model selection matters', 
                 fontsize=12)
    ax.grid(axis='x', alpha=0.3)
    
    # Add values
    for i, row in enumerate(range_df.itertuples()):
        ax.text(row.range + 0.01, i, f'{row.range:.3f}', 
               va='center', fontsize=9)
    
    # Plot 2: Winner by dataset
    ax = axes[1]
    
    winners = []
    for ds in datasets:
        ds_data = summary[summary['Dataset'] == ds].sort_values('F1', ascending=False)
        winner = ds_data.iloc[0]['Model']
        f1_win = ds_data.iloc[0]['F1']
        f1_mean = ds_data['F1'].mean()
        winners.append({'Dataset': ds, 'Winner': winner, 
                       'F1': f1_win, 'Difficulty': f1_mean})
    
    win_df = pd.DataFrame(winners).sort_values('Difficulty', ascending=False)
    
    colors_map = {'ARIMA': 'blue', 'Toto': 'green', 'GraniteTTM': 'red'}
    colors = [colors_map.get(w, 'gray') for w in win_df['Winner']]
    
    x = range(len(win_df))
    ax.barh(x, win_df['F1'], color=colors, alpha=0.7, edgecolor='black')
    ax.set_yticks(x)
    ax.set_yticklabels(win_df['Dataset'])
    ax.set_xlabel('Best F1 Score', fontsize=11)
    ax.set_title('Best Model per Dataset\n(Color = winning model)', fontsize=12)
    ax.grid(axis='x', alpha=0.3)
    
    # Legend
    from matplotlib.patches import Patch
    legend_elements = [Patch(facecolor=colors_map[m], label=m, alpha=0.7) 
                      for m in ['ARIMA', 'Toto', 'GraniteTTM']]
    ax.legend(handles=legend_elements, loc='lower right')
    
    plt.tight_layout()
    plt.savefig(output_dir / 'dataset_difficulty_analysis.png', 
                dpi=150, bbox_inches='tight')
    plt.close()
    
    return difficulty, win_df


def analysis_3_coverage_vs_detection(summary, output_dir):
    """
    Test hypothesis: Over-covered intervals ‚Üí fewer anomalies detected
    Under-covered intervals ‚Üí more false positives
    """
    output_dir = Path(output_dir)
    
    print("\n" + "="*80)
    print("ANALYSIS 3: PREDICTION INTERVAL COVERAGE vs DETECTION")
    print("="*80)
    
    # Calculate deviation from target (90%)
    summary_copy = summary.copy()
    summary_copy['coverage_error'] = summary_copy['Coverage'] - 0.90
    summary_copy['coverage_abs_error'] = np.abs(summary_copy['coverage_error'])
    
    fig, axes = plt.subplots(1, 2, figsize=(14, 6))
    
    # Plot 1: Coverage error vs F1
    ax = axes[0]
    colors_map = {'ARIMA': 'blue', 'Toto': 'green', 'GraniteTTM': 'red'}
    
    for model in summary_copy['Model'].unique():
        df_m = summary_copy[summary_copy['Model'] == model]
        ax.scatter(df_m['coverage_error'], df_m['F1'],
                  label=model, s=120, alpha=0.7,
                  color=colors_map.get(model, 'gray'),
                  edgecolors='black')
    
    ax.axvline(0, color='black', linestyle='--', alpha=0.5, label='Perfect calibration')
    ax.set_xlabel('Coverage Error (actual - 0.90 target)', fontsize=11)
    ax.set_ylabel('F1 Score', fontsize=11)
    ax.set_title('Impact of Interval Calibration on Detection\n' +
                 'Under-coverage (<0) vs Over-coverage (>0)', fontsize=12)
    ax.grid(True, alpha=0.3)
    ax.legend()
    
    # Correlation
    corr = summary_copy[['coverage_abs_error', 'F1']].corr().iloc[0, 1]
    ax.text(0.05, 0.95, f'|Coverage Error| vs F1\nr = {corr:+.3f}', 
           transform=ax.transAxes, fontsize=10,
           verticalalignment='top',
           bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
    
    # Plot 2: Model comparison
    ax = axes[1]
    
    model_stats = summary_copy.groupby('Model').agg({
        'Coverage': ['mean', 'std'],
        'F1': 'mean'
    }).round(3)
    
    models = model_stats.index
    coverage_means = model_stats[('Coverage', 'mean')].values
    coverage_stds = model_stats[('Coverage', 'std')].values
    
    x = range(len(models))
    bars = ax.bar(x, coverage_means, yerr=coverage_stds, 
                  capsize=5, alpha=0.7, edgecolor='black',
                  color=[colors_map.get(m, 'gray') for m in models])
    
    ax.axhline(0.90, color='red', linestyle='--', linewidth=2, label='Target (90%)')
    ax.set_xticks(x)
    ax.set_xticklabels(models)
    ax.set_ylabel('Prediction Interval Coverage', fontsize=11)
    ax.set_title('Model Calibration Quality\n(closer to 90% = better)', fontsize=12)
    ax.legend()
    ax.grid(axis='y', alpha=0.3)
    
    # Add value labels
    for i, (mean, std) in enumerate(zip(coverage_means, coverage_stds)):
        ax.text(i, mean + std + 0.01, f'{mean:.2%}',
               ha='center', va='bottom', fontsize=10, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig(output_dir / 'coverage_vs_detection.png', 
                dpi=150, bbox_inches='tight')
    plt.close()
    
    print("\nüìä COVERAGE STATISTICS:")
    print(model_stats.to_string())
    
    print(f"\nüîç COVERAGE vs DETECTION:")
    print(f"  Correlation (|coverage_error| vs F1): r = {corr:+.3f}")
    
    if abs(corr) < 0.3:
        print("  ‚Üí WEAK correlation: Well-calibrated intervals don't guarantee good detection")
        print("  ‚Üí Detection quality depends more on residual patterns than interval width")
    else:
        print("  ‚Üí MODERATE correlation: Calibration affects detection")
        print("  ‚Üí Over-covered intervals may hide anomalies in 'normal' range")
    
    return summary_copy


def analysis_4_statistical_significance(summary, output_dir):
    """
    Test if model performance differences are statistically significant
    """
    output_dir = Path(output_dir)
    
    print("\n" + "="*80)
    print("ANALYSIS 4: STATISTICAL SIGNIFICANCE OF MODEL DIFFERENCES")
    print("="*80)
    
    # Paired t-tests (same datasets for each model)
    models = summary['Model'].unique()
    
    results = []
    for i, m1 in enumerate(models):
        for m2 in models[i+1:]:
            # Get F1 scores for same datasets
            datasets = summary['Dataset'].unique()
            f1_m1 = []
            f1_m2 = []
            
            for ds in datasets:
                val1 = summary[(summary['Dataset']==ds) & (summary['Model']==m1)]['F1']
                val2 = summary[(summary['Dataset']==ds) & (summary['Model']==m2)]['F1']
                
                if len(val1) > 0 and len(val2) > 0:
                    f1_m1.append(val1.values[0])
                    f1_m2.append(val2.values[0])
            
            if len(f1_m1) >= 3:  # Need at least 3 pairs
                t_stat, p_val = stats.ttest_rel(f1_m1, f1_m2)
                mean_diff = np.mean(f1_m1) - np.mean(f1_m2)
                
                results.append({
                    'Model_1': m1,
                    'Model_2': m2,
                    'Mean_F1_1': np.mean(f1_m1),
                    'Mean_F1_2': np.mean(f1_m2),
                    'Difference': mean_diff,
                    't_statistic': t_stat,
                    'p_value': p_val,
                    'Significant_05': 'Yes' if p_val < 0.05 else 'No',
                    'n_datasets': len(f1_m1)
                })
    
    sig_df = pd.DataFrame(results)
    sig_df.to_csv(output_dir / 'statistical_significance.csv', index=False)
    
    print("\nüìä PAIRWISE COMPARISONS (Paired t-tests on F1):")
    print(sig_df.to_string(index=False))
    
    print("\nüîç INTERPRETATION:")
    sig_count = (sig_df['p_value'] < 0.05).sum()
    if sig_count == 0:
        print("  ‚ö†Ô∏è  NO significant differences detected (all p > 0.05)")
        print("  ‚Üí Models are statistically equivalent on this benchmark")
        print("  ‚Üí Choose based on speed, calibration, or specific use case")
    else:
        print(f"  ‚úì {sig_count} significant difference(s) found (p < 0.05)")
        for _, row in sig_df[sig_df['p_value'] < 0.05].iterrows():
            winner = row['Model_1'] if row['Difference'] > 0 else row['Model_2']
            print(f"  ‚Üí {winner} significantly outperforms (p={row['p_value']:.3f})")
    
    return sig_df


def generate_executive_summary(summary, sep_df, difficulty, sig_df, output_dir):
    """
    Create a comprehensive markdown summary of all new findings
    """
    output_dir = Path(output_dir)
    
    # Calculate key statistics
    corr_forecast = summary[['sMAPE', 'F1']].corr().iloc[0, 1]
    corr_separability = sep_df[['auc_residual', 'F1']].corr().iloc[0, 1]
    
    hardest_dataset = difficulty.iloc[-1].name
    easiest_dataset = difficulty.iloc[0].name
    
    report = f"""# Enhanced Analysis Results
## Time-Series Anomaly Detection Benchmark - Progress Update

Generated: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}

---

## üéØ Key New Findings

### 1. Forecast Quality vs Residual Separability

**The Core Question**: Does better forecasting help detection by creating more separable residuals?

| Metric | Correlation with F1 | Interpretation |
|--------|--------------------:|----------------|
| Forecast Quality (sMAPE) | {corr_forecast:+.3f} | {'Strong' if abs(corr_forecast) > 0.5 else 'Moderate'} negative |
| Residual Separability (AUC) | {corr_separability:+.3f} | {'Strong' if abs(corr_separability) > 0.5 else 'Moderate'} positive |

"""
    
    if abs(corr_separability) > abs(corr_forecast):
        report += """
**Conclusion**: ‚úÖ Residual separability predicts detection success better than raw forecast accuracy.

**Implications**:
- The detector is successfully exploiting residual patterns
- Better forecasting ‚Üí better residuals ‚Üí better detection (causal chain validated)
- Continue improving forecasting models, but also optimize detector for specific residual distributions

"""
    else:
        report += """
**Conclusion**: ‚ö†Ô∏è Forecast accuracy predicts detection better than residual separability.

**Implications**:
- Detector has fundamental limitations beyond residual quality
- Even well-separated residuals aren't being fully exploited
- **Priority**: Improve detection algorithm, not just forecasting
- Consider: adaptive thresholds, multivariate detection, attention mechanisms

"""
    
    report += f"""
### 2. Dataset Difficulty Analysis

**Hardest Dataset**: `{hardest_dataset}` (mean F1 = {difficulty.loc[hardest_dataset, 'F1_mean']:.3f})
**Easiest Dataset**: `{easiest_dataset}` (mean F1 = {difficulty.loc[easiest_dataset, 'F1_mean']:.3f})

Datasets with **high F1 range** (max-min) indicate model selection matters:
"""
    
    # Add top 3 datasets where model choice matters most
    range_data = []
    for ds in summary['Dataset'].unique():
        f1_vals = summary[summary['Dataset'] == ds]['F1'].values
        range_data.append({
            'Dataset': ds,
            'F1_range': np.max(f1_vals) - np.min(f1_vals),
            'Winner': summary[summary['Dataset'] == ds].sort_values('F1', ascending=False).iloc[0]['Model']
        })
    
    range_df = pd.DataFrame(range_data).sort_values('F1_range', ascending=False)
    
    report += "\n| Dataset | F1 Range | Best Model |\n"
    report += "|---------|----------|------------|\n"
    for _, row in range_df.head(3).iterrows():
        report += f"| {row['Dataset']} | {row['F1_range']:.3f} | {row['Winner']} |\n"
    
    report += f"""

### 3. Statistical Significance

Total pairwise comparisons: {len(sig_df)}
Significant differences (p < 0.05): {(sig_df['p_value'] < 0.05).sum()}

"""
    
    if (sig_df['p_value'] < 0.05).any():
        report += "**Significant findings**:\n"
        for _, row in sig_df[sig_df['p_value'] < 0.05].iterrows():
            winner = row['Model_1'] if row['Difference'] > 0 else row['Model_2']
            loser = row['Model_2'] if row['Difference'] > 0 else row['Model_1']
            report += f"- {winner} > {loser}: Œî={abs(row['Difference']):.3f}, p={row['p_value']:.3f}\n"
    else:
        report += """
**Conclusion**: No statistically significant differences between models.

**Implications**:
- All three models perform equivalently on average
- Model selection should prioritize: inference speed, calibration, operational requirements
- With only 6 datasets, power is limited‚Äî15-dataset run will clarify

"""
    
    report += """

---

## üìä Visualizations Generated

1. **separability_vs_f1_comprehensive.png** - Three-panel analysis of residual separability metrics
2. **dataset_difficulty_analysis.png** - Difficulty ranking and winning models per dataset
3. **coverage_vs_detection.png** - Impact of interval calibration on detection
4. **statistical_significance.csv** - Complete pairwise comparison table

---

## üöÄ Next Steps

### Immediate (Next Run)
1. ‚úÖ Complete 15-dataset benchmark to increase statistical power
2. Test alternative detectors (Isolation Forest, LSTM autoencoder) on same residuals
3. Implement ensemble forecasting (average of all three models)

### Short-term (This Month)
1. Pattern-based model routing (use STL decomposition to characterize datasets)
2. Hyperparameter tuning for Granite (context length: 512, 1024, 2048)
3. Multi-step ahead forecasting (current: 1-step)

### Research Questions Answered
- ‚úÖ Does forecast quality matter? **Yes** (r = {corr_forecast:.3f})
- ‚úÖ Do models create separable residuals? **Varies by dataset**
- ‚úÖ Are differences significant? **{"Yes, some pairs" if (sig_df['p_value'] < 0.05).any() else "Not yet with n=6"}**

### Research Questions Remaining
- ‚ùì Which patterns does each model excel on? (need STL characterization)
- ‚ùì Can ensemble beat individual models? (need to implement)
- ‚ùì Do multivariate methods help? (need multi-channel datasets)

---

## üìÅ Output Files

All analysis artifacts saved to: `{output_dir}/`

"""
    
    report_path = output_dir / "PROGRESS_REPORT.md"
    report_path.write_text(report)
    print(f"\nüìÑ Executive summary saved: {report_path}")
    
    return report


# ============================================================
# MAIN EXECUTION
# ============================================================

def run_enhanced_analysis(run_dir):
    """
    Run all enhanced analyses on existing benchmark results
    
    Args:
        run_dir: Path to your TSB_* output directory
    """
    run_dir = Path(run_dir)
    output_dir = run_dir / "enhanced_analysis"
    output_dir.mkdir(exist_ok=True)
    
    print("\n" + "="*80)
    print("ENHANCED ANALYSIS PIPELINE")
    print("="*80)
    print(f"Input:  {run_dir}")
    print(f"Output: {output_dir}")
    
    # Load results
    print("\nüìÇ Loading results...")
    summary, traces = load_results(run_dir)
    print(f"  ‚úì Loaded {len(summary)} result rows")
    print(f"  ‚úì Loaded {len(traces)} trace files")
    
    # Run analyses
    sep_df = analysis_1_separability_vs_detection(summary, traces, output_dir)
    difficulty, win_df = analysis_2_dataset_difficulty(summary, output_dir)
    summary_cov = analysis_3_coverage_vs_detection(summary, output_dir)
    sig_df = analysis_4_statistical_significance(summary, output_dir)
    
    # Generate summary report
    print("\nüìù Generating executive summary...")
    report = generate_executive_summary(summary, sep_df, difficulty, sig_df, output_dir)
    
    print("\n" + "="*80)
    print("‚úÖ ENHANCED ANALYSIS COMPLETE")
    print("="*80)
    print(f"\nAll outputs saved to: {output_dir}/")
    print("\nKey files:")
    print("  - PROGRESS_REPORT.md (executive summary)")
    print("  - separability_analysis.csv")
    print("  - dataset_difficulty.csv")
    print("  - statistical_significance.csv")
    print("  - *.png (visualization plots)")
    
    return {
        'summary': summary,
        'separability': sep_df,
        'difficulty': difficulty,
        'significance': sig_df,
        'output_dir': output_dir
    }


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("Usage: python enhanced_analysis.py <path_to_TSB_run_directory>")
        print("\nExample:")
        print("  python enhanced_analysis.py runs/TSB_2025-09-28_15-30-00_cu124_toto128_lim6_hostname")
        sys.exit(1)
    
    run_dir = sys.argv[1]
    results = run_enhanced_analysis(run_dir)
    
    print("\nüéâ Analysis complete
