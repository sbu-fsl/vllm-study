# LLM Benchmarks

6 separate tasks, using 6 different datasets. Using OpenAI API to send our requests. Sampling 20 data from these datasets for our benchmarks.

## Datasets

- [MMLU (Massive Multitask Language Understanding)](https://huggingface.co/datasets/cais/mmlu)
    - Benchmark for GPT-scale models; used in many evaluation papers.
- [HumanEval](https://huggingface.co/datasets/openai/openai_humaneval)
    - By OpenAI.
- [Natural Questions](https://huggingface.co/datasets/google-research-datasets/natural_questions)
    - By Google research, Long-form question answering from real Google search queries + Wikipedia.
- [LooGLE](https://huggingface.co/datasets/bigai-nlco/LooGLE)
    - [Long documents + QA designed to test context >24k tokens.](https://arxiv.org/abs/2311.04939)
- [QMSum](https://huggingface.co/datasets/pszemraj/qmsum-cleaned)
    - Academic/industry collaborations for meeting summarization.
- [OpenChat](https://huggingface.co/datasets/openchat/openchat_sharegpt4_dataset)
    - Cleaned GPT-4-based ShareGPT data used for training OpenChat.
- [Alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca)
    - A dataset of 52,000 instructions and demonstrations generated by OpenAI's text-davinci-003 engine.

## Tasks

- Single Prompt Single Response
    - HumanEval
    - Alpaca
- Beam Search Evaluation
    - Natural Questions
- Shared Prefix
    - LooGLE
- Chatbot Evaluation
    - OpenChat
- Question Answering
    - MMLU
- Summarization
    - QMSum
