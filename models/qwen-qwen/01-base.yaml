# Base deployment for Qwen/Qwen2.5-1.5B-Instruct
# Node: SUNY/iBM 1
# Storage: local
# Version: v0.1
apiVersion: v1
kind: Pod
metadata:
  name: qwen-qwen
  namespace: llm-servings
  labels:
    app: vllm
    model: qwen-qwen
    serviceid: "1"
spec:
  tolerations: # must have this to schedule on GPU nodes
  - key: "level"
    operator: "Equal"
    value: "important"
    effect: "NoSchedule"

  nodeSelector: # RTX A5000 with 24.5 GB of VRAM
    kubernetes.io/hostname: sunyibm1.fsl.cs.sunysb.edu

  restartPolicy: Never

  volumes: # a host path volume
  - name: cache-volume
    persistentVolumeClaim:
      claimName: vllm-local-pvc
  - name: shm
    emptyDir:
      medium: Memory
      sizeLimit: "1G"

  containers:
  - name: vllm-container
    image: vllm/vllm-openai:latest
    command: ["/bin/sh", "-c"]
    args:
      - >
        vllm serve Qwen/Qwen2.5-1.5B-Instruct
        --host 0.0.0.0
        --port 8000
        --tensor-parallel-size 1
        --no-enable-prefix-caching
        --trust-remote-code
        --seed 0
        --gpu-memory-utilization 0.95

    env:
    - name: HF_TOKEN
      valueFrom:
        secretKeyRef:
          name: hf-token-secret
          key: token

    ports:
    - containerPort: 8000

    resources:
      limits:
        cpu: "10"
        memory: 25G
        nvidia.com/gpu: "1"
      requests:
        cpu: "5"
        memory: 12G
        nvidia.com/gpu: "1"

    volumeMounts:
    - mountPath: /root/.cache/huggingface
      name: cache-volume
    - name: shm
      mountPath: /dev/shm

    livenessProbe:
      httpGet:
        path: /health
        port: 8000
      initialDelaySeconds: 120
      periodSeconds: 60

    readinessProbe:
      httpGet:
        path: /health
        port: 8000
      initialDelaySeconds: 120
      periodSeconds: 60
