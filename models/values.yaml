## NOTE: This file is template for model values configuration.
## Please create a copy of this file and fill in the values accordingly.

# namespace configs
namespace: llm-servings

# model configs
# NOTES:
# - replace all `placeholder` parts
# - must set tensor-parallel-size to the gpu.count
# - remove max-model-len if it's not needed
# - remove the kv-transfer-config and disable lmcache if its not needed
configs:
  name: name-placeholder
  serve:
    - >
      vllm serve model-placeholder
      --host 0.0.0.0
      --port 8000
      --tensor-parallel-size 1
      --enable-prefix-caching
      --seed 0
      --gpu-memory-utilization 0.9
      --max-model-len 1000
      --kv-transfer-config '{"kv_connector":"LMCacheConnectorV1", "kv_role":"kv_both"}'
  lmcache:
    enable: true
    cache_type: "local_storage"

# gpu configs
# RTX A5000 with 24.5 GB of VRAM (1) / Quadro 6000 with 24.5 GB of VRAM (2)
gpu:
  type: "q6000" # must be a5000 or q6000
  count: 1   # adjust this based on tensor-parallel-size

# storage configs
storage:
  type: "local" # must be local or gpfs

# Kubernetes liveness and readiness probes
liveprobe:
  initdelay: 300
  period: 300
readyprobe:
  initdelay: 300
  period: 300
