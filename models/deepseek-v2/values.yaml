# namespace configs
namespace: llm-servings

# model configs
configs:
  name: deepseek-v2.5
  serve:
    - >
      vllm serve deepseek-ai/DeepSeek-V2.5
      --host 0.0.0.0
      --port 8000
      --tensor-parallel-size 1
      --no-enable-prefix-caching
      --trust-remote-code
      --seed 0
      --gpu-memory-utilization 0.95

# service id
serviceid: 1
