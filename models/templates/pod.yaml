---
apiVersion: v1
kind: Pod
metadata:
  name: {{ .Values.configs.name }}
  namespace: {{ .Values.namespace }}
  labels:
    app: vllm
    model: {{ .Values.configs.name }}
spec:
  tolerations: # must have this to schedule on GPU nodes
  - key: "level"
    operator: "Equal"
    value: "important"
    effect: "NoSchedule"

  nodeSelector:
    {{- if eq .Values.gpu.type "a5000" }}
    kubernetes.io/hostname: sunyibm1.fsl.cs.sunysb.edu
    {{- else if eq .Values.gpu.type "q6000" }}
    kubernetes.io/hostname: sunyibm2.fsl.cs.sunysb.edu
    {{- else }}
    {{ fail "Invalid gpu type value. Use 'a5000' or 'q6000'." }}
    {{- end }}

  restartPolicy: Never

  volumes:
  - name: cache-volume
    {{- if eq .Values.storage.type "local" }}
    hostPath:
      path: /mnt/vllm
      type: DirectoryOrCreate
    {{- else if eq .Values.storage.type "gpfs" }}
    persistentVolumeClaim:
      claimName: vllm-pvc
    {{- else }}
    {{ fail "Invalid storage type value. Use 'local' or 'gpfs'." }}
    {{- end }}
  - name: shm
    emptyDir:
      medium: Memory
      sizeLimit: "1G"

  containers:
  - name: vllm-container
    image: vllm/vllm-openai:v0.15.0
    command: ["/bin/sh", "-c"]
    args: {{ .Values.configs.serve }}

    env:
    - name: HF_TOKEN
      valueFrom:
        secretKeyRef:
          name: hf-token-secret
          key: token

    ports:
    - containerPort: 8000

    resources:
      limits:
        cpu: "10"
        memory: 25G
        nvidia.com/gpu: "1"
      requests:
        cpu: "5"
        memory: 12G
        nvidia.com/gpu: "1"

    volumeMounts:
    - mountPath: /root/.cache/huggingface
      name: cache-volume
    - name: shm
      mountPath: /dev/shm

    livenessProbe:
      httpGet:
        path: /health
        port: 8000
      initialDelaySeconds: 120
      periodSeconds: 60

    readinessProbe:
      httpGet:
        path: /health
        port: 8000
      initialDelaySeconds: 120
      periodSeconds: 60
