apiVersion: batch/v1
kind: Job
metadata:
  name: ml-workload
  namespace: ml-scheduling
spec:
  template:
    spec:
      restartPolicy: Never
      containers:
      - name: ml-workload
        image: pytorch/pytorch:2.2.0-cuda11.8-cudnn8-runtime
        tty: true
        stdin: true
        command: ["/bin/bash", "-c"]
        args:
          - |
            set -e
            # ---- system tools ----
            apt-get update -qq && \
            DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends \
                tmux nano curl ca-certificates && \
            curl -sL https://github.com/zyedidia/micro/releases/download/v2.0.14/micro-2.0.14-linux64-static.tar.gz \
              | tar -xz --no-same-owner -C /usr/local/bin micro-2.0.14/micro && \
            chmod +x /usr/local/bin/micro && \
            apt-get clean && rm -rf /var/lib/apt/lists/*

            # ---- python deps ----
            pip install --no-cache-dir --upgrade pip && \
            pip install --no-cache-dir transformers accelerate

            echo "Pod ready â€” attach with: kubectl exec -it <pod> -- bash"
            exec sleep infinity
        env:
        - name: NUM_EPOCHS
          value: "100"
        - name: MASTER_ADDR
          value: "127.0.0.1"
        - name: MASTER_PORT
          value: "29500"
        resources:
          requests:
            nvidia.com/gpu: 4
            ephemeral-storage: "20Gi"
          limits:
            nvidia.com/gpu: 4
            ephemeral-storage: "20Gi"
        volumeMounts:
        - name: workspace-storage
          mountPath: /home
        - name: dshm
          mountPath: /dev/shm
      volumes:
      - name: workspace-storage
        persistentVolumeClaim:
          claimName: pvc-workspace
      - name: dshm
        emptyDir:
          medium: Memory
          sizeLimit: 20Gi
