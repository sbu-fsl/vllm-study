{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "037699bd",
   "metadata": {},
   "source": [
    "# Time Series Comparison: Correlation, Derivatives, DTW, and PCA\n",
    "\n",
    "Compare multiple time series from different workloads using four complementary analysis methods:\n",
    "1. **Correlation Heatmap** - Overall similarity\n",
    "2. **Derivative Correlation** - Rate-of-change similarity\n",
    "3. **DTW Distance Matrix** - Temporal pattern similarity\n",
    "4. **PCA Scatter Plot** - Feature-based clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9146eee5",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f24aaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from scipy import interpolate, signal\n",
    "from scipy.stats import zscore, pearsonr\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Try to import dtaidistance for faster DTW computation\n",
    "try:\n",
    "    from dtaidistance import dtw\n",
    "    HAS_DTAIDISTANCE = True\n",
    "except ImportError:\n",
    "    HAS_DTAIDISTANCE = False\n",
    "    print(\"Note: dtaidistance not available, will use slower DTW implementation\")\n",
    "\n",
    "# Set visual style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36d9a21",
   "metadata": {},
   "source": [
    "## 2. Load and Explore CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da2b13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATASET_DIR = Path(\"/Users/anajafizadeh/projects/sunyibm/vllm-study/timeseries-models/vllm_datasets\")\n",
    "OUTPUT_DIR = Path(\"/Users/anajafizadeh/projects/sunyibm/vllm-study/plots/results\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Find available CSV files\n",
    "csv_files = sorted(list(DATASET_DIR.glob(\"*.csv\")))\n",
    "print(f\"Found {len(csv_files)} CSV files in {DATASET_DIR}\")\n",
    "print(\"\\nAvailable files:\")\n",
    "for i, f in enumerate(csv_files, 1):\n",
    "    print(f\"  {i:2d}. {f.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e16c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select files to compare (modify this list as needed)\n",
    "# Option 1: Compare first N files\n",
    "N_FILES = 6\n",
    "selected_files = csv_files[:N_FILES]\n",
    "\n",
    "# Option 2: Compare specific files\n",
    "# selected_files = [\n",
    "#     DATASET_DIR / \"CPU_Usage-data-2025-10-02_00_22_34.csv\",\n",
    "#     DATASET_DIR / \"Memory_Usage-data-2025-10-02_00_22_44.csv\",\n",
    "# ]\n",
    "\n",
    "print(f\"\\nSelected {len(selected_files)} files for comparison:\")\n",
    "for f in selected_files:\n",
    "    print(f\"  - {f.stem}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c350fe15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and explore data\n",
    "dataframes = {}\n",
    "for csv_file in selected_files:\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file, usecols=[\"timestamp\", \"value\"])\n",
    "        if not df.empty:\n",
    "            df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "            dataframes[csv_file.stem] = df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {csv_file.name}: {e}\")\n",
    "\n",
    "print(f\"\\nSuccessfully loaded {len(dataframes)} datasets\")\n",
    "print(\"\\nDataset statistics:\")\n",
    "for name, df in dataframes.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Shape: {df.shape}\")\n",
    "    print(f\"  Value range: [{df['value'].min():.2f}, {df['value'].max():.2f}]\")\n",
    "    print(f\"  Mean: {df['value'].mean():.2f}, Std: {df['value'].std():.2f}\")\n",
    "    print(f\"  Sample:\\n{df.head(3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1dc14e",
   "metadata": {},
   "source": [
    "## 3. Normalize and Align Time Series Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4577f9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create common time grid and align all series\n",
    "min_time = min(df[\"timestamp\"].min() for df in dataframes.values())\n",
    "max_time = max(df[\"timestamp\"].max() for df in dataframes.values())\n",
    "\n",
    "# Convert to seconds since start\n",
    "aligned_series = {}\n",
    "time_offsets = {}\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    t_norm = (df[\"timestamp\"] - df[\"timestamp\"].iloc[0]).dt.total_seconds().values\n",
    "    values = df[\"value\"].values\n",
    "    time_offsets[name] = (df[\"timestamp\"].iloc[0] - min_time).total_seconds()\n",
    "    \n",
    "# Create common time grid\n",
    "total_time = (max_time - min_time).total_seconds()\n",
    "n_points = max(1000, max(len(df) for df in dataframes.values()))\n",
    "time_grid = np.linspace(0, total_time, n_points)\n",
    "\n",
    "print(f\"Common time grid: {n_points} points from 0 to {total_time:.1f} seconds\")\n",
    "print(f\"\\nInterpolating all series to common grid...\")\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    t_norm = (df[\"timestamp\"] - df[\"timestamp\"].iloc[0]).dt.total_seconds().values\n",
    "    values = df[\"value\"].values\n",
    "    offset = time_offsets[name]\n",
    "    \n",
    "    # Shift time by offset\n",
    "    t_shifted = t_norm + offset\n",
    "    \n",
    "    # Interpolate\n",
    "    f = interpolate.interp1d(t_shifted, values, kind='cubic', \n",
    "                            bounds_error=False, fill_value='extrapolate')\n",
    "    interpolated = f(time_grid)\n",
    "    \n",
    "    # Clip to reasonable values (handle extrapolation artifacts)\n",
    "    interpolated = np.clip(interpolated, np.percentile(values, 1), np.percentile(values, 99))\n",
    "    \n",
    "    # Normalize (z-score)\n",
    "    aligned_series[name] = (interpolated - np.mean(interpolated)) / (np.std(interpolated) + 1e-10)\n",
    "\n",
    "print(f\"✓ Aligned {len(aligned_series)} series to common grid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dfc889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize aligned time series\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "for name, series in aligned_series.items():\n",
    "    ax.plot(time_grid, series, label=name, linewidth=1.5, alpha=0.8)\n",
    "ax.set_xlabel('Time (seconds)', fontsize=12)\n",
    "ax.set_ylabel('Normalized Value', fontsize=12)\n",
    "ax.set_title('Aligned Time Series (Normalized)', fontsize=14, fontweight='bold')\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"00_aligned_time_series.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"✓ Saved to {OUTPUT_DIR / '00_aligned_time_series.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee873e9c",
   "metadata": {},
   "source": [
    "## 4. Generate Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc241557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Pearson correlation matrix\n",
    "labels = list(aligned_series.keys())\n",
    "n = len(labels)\n",
    "corr_matrix = np.zeros((n, n))\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        s1 = aligned_series[labels[i]]\n",
    "        s2 = aligned_series[labels[j]]\n",
    "        corr_matrix[i, j] = np.corrcoef(s1, s2)[0, 1]\n",
    "\n",
    "print(\"Correlation Matrix:\")\n",
    "print(pd.DataFrame(corr_matrix, index=labels, columns=labels).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4730f129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize correlation heatmap\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap=\"RdBu_r\",\n",
    "           xticklabels=labels, yticklabels=labels,\n",
    "           vmin=-1, vmax=1, cbar_kws={\"label\": \"Correlation\"},\n",
    "           ax=ax, square=True, linewidths=0.5)\n",
    "ax.set_title('Correlation Heatmap (Normalized)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"01_correlation_heatmap.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"✓ Saved to {OUTPUT_DIR / '01_correlation_heatmap.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f8d380",
   "metadata": {},
   "source": [
    "## 5. Calculate and Visualize Derivative Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c092d920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute derivatives (rate of change)\n",
    "derivatives = {}\n",
    "for name, series in aligned_series.items():\n",
    "    deriv = np.gradient(series)\n",
    "    derivatives[name] = deriv\n",
    "\n",
    "# Compute correlation matrix of derivatives\n",
    "deriv_corr_matrix = np.zeros((n, n))\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        d1 = derivatives[labels[i]]\n",
    "        d2 = derivatives[labels[j]]\n",
    "        # Normalize derivatives\n",
    "        d1_norm = (d1 - np.mean(d1)) / (np.std(d1) + 1e-10)\n",
    "        d2_norm = (d2 - np.mean(d2)) / (np.std(d2) + 1e-10)\n",
    "        deriv_corr_matrix[i, j] = np.corrcoef(d1_norm, d2_norm)[0, 1]\n",
    "\n",
    "print(\"Derivative Correlation Matrix:\")\n",
    "print(pd.DataFrame(deriv_corr_matrix, index=labels, columns=labels).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fafd8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize derivative correlation heatmap\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(deriv_corr_matrix, annot=True, fmt=\".2f\", cmap=\"RdBu_r\",\n",
    "           xticklabels=labels, yticklabels=labels,\n",
    "           vmin=-1, vmax=1, cbar_kws={\"label\": \"Correlation\"},\n",
    "           ax=ax, square=True, linewidths=0.5)\n",
    "ax.set_title('Derivative Correlation Heatmap (Rate of Change)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"02_derivative_correlation_heatmap.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"✓ Saved to {OUTPUT_DIR / '02_derivative_correlation_heatmap.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c0bb81",
   "metadata": {},
   "source": [
    "## 6. Compute DTW Distance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed10dfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dtw_distance_simple(series1, series2):\n",
    "    \"\"\"Compute Dynamic Time Warping distance using dynamic programming.\"\"\"\n",
    "    n, m = len(series1), len(series2)\n",
    "    dtw_matrix = np.full((n + 1, m + 1), np.inf)\n",
    "    dtw_matrix[0, 0] = 0\n",
    "    \n",
    "    for i in range(1, n + 1):\n",
    "        for j in range(1, m + 1):\n",
    "            cost = np.abs(series1[i-1] - series2[j-1])\n",
    "            dtw_matrix[i, j] = cost + min(\n",
    "                dtw_matrix[i-1, j],\n",
    "                dtw_matrix[i, j-1],\n",
    "                dtw_matrix[i-1, j-1]\n",
    "            )\n",
    "    \n",
    "    return dtw_matrix[n, m]\n",
    "\n",
    "print(\"Computing DTW distances... (this may take a moment)\")\n",
    "\n",
    "dtw_matrix = np.zeros((n, n))\n",
    "for i in range(n):\n",
    "    for j in range(i, n):\n",
    "        if HAS_DTAIDISTANCE:\n",
    "            dist = dtw.distance_fast(aligned_series[labels[i]], aligned_series[labels[j]])\n",
    "        else:\n",
    "            dist = dtw_distance_simple(aligned_series[labels[i]], aligned_series[labels[j]])\n",
    "        dtw_matrix[i, j] = dist\n",
    "        dtw_matrix[j, i] = dist\n",
    "        if (i+1) % 2 == 0:\n",
    "            print(f\"  Progress: {i+1}/{n}\", end='\\r')\n",
    "\n",
    "print(\"\\n✓ DTW computation complete\")\n",
    "print(\"\\nDTW Distance Matrix:\")\n",
    "print(pd.DataFrame(dtw_matrix, index=labels, columns=labels).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36b713d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize DTW distances for better visualization\n",
    "dtw_matrix_norm = dtw_matrix / np.max(dtw_matrix)\n",
    "\n",
    "# Visualize DTW distance matrix\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(dtw_matrix_norm, annot=True, fmt=\".2f\", cmap=\"YlOrRd\",\n",
    "           xticklabels=labels, yticklabels=labels,\n",
    "           cbar_kws={\"label\": \"Normalized DTW Distance\"},\n",
    "           ax=ax, square=True, linewidths=0.5)\n",
    "ax.set_title('DTW Distance Matrix (Temporal Patterns)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"03_dtw_distance_matrix.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"✓ Saved to {OUTPUT_DIR / '03_dtw_distance_matrix.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d729113",
   "metadata": {},
   "source": [
    "## 7. Perform PCA and Create Scatter Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340fcb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 12 statistical features from each time series\n",
    "def extract_features(series):\n",
    "    mean_val = np.mean(series)\n",
    "    std_val = np.std(series)\n",
    "    \n",
    "    features = np.array([\n",
    "        mean_val,\n",
    "        std_val,\n",
    "        np.min(series),\n",
    "        np.max(series),\n",
    "        np.median(series),\n",
    "        np.percentile(series, 25),\n",
    "        np.percentile(series, 75),\n",
    "        np.mean(np.abs(np.gradient(series))),  # mean absolute derivative\n",
    "        np.std(np.gradient(series)),  # std of derivative\n",
    "        (np.max(series) - np.min(series)) / (mean_val + 1e-10),  # coefficient of variation\n",
    "    ])\n",
    "    return features\n",
    "\n",
    "# Extract features for all series\n",
    "features_list = []\n",
    "for name in labels:\n",
    "    features_list.append(extract_features(aligned_series[name]))\n",
    "\n",
    "features_array = np.array(features_list)\n",
    "feature_names = ['Mean', 'Std', 'Min', 'Max', 'Median', 'Q1', 'Q3', 'Mean|Deriv|', 'Std(Deriv)', 'CoV']\n",
    "\n",
    "# Show feature values\n",
    "print(\"Extracted Features:\")\n",
    "df_features = pd.DataFrame(features_array, index=labels, columns=feature_names)\n",
    "print(df_features.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea2b925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features and apply PCA\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features_array)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "features_pca = pca.fit_transform(features_scaled)\n",
    "\n",
    "print(f\"PCA Explained Variance Ratio:\")\n",
    "for i, var in enumerate(pca.explained_variance_ratio_):\n",
    "    print(f\"  PC{i+1}: {var:.1%}\")\n",
    "print(f\"  Total: {sum(pca.explained_variance_ratio_):.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933a1c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PCA scatter plot\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "scatter = ax.scatter(features_pca[:, 0], features_pca[:, 1],\n",
    "                     s=300, alpha=0.7, c=range(len(labels)),\n",
    "                     cmap='tab10', edgecolors='black', linewidth=2)\n",
    "\n",
    "# Add labels\n",
    "for i, label in enumerate(labels):\n",
    "    ax.annotate(label, (features_pca[i, 0], features_pca[i, 1]),\n",
    "               xytext=(10, 10), textcoords='offset points',\n",
    "               fontsize=9, bbox=dict(boxstyle='round,pad=0.5',\n",
    "               facecolor='yellow', alpha=0.4))\n",
    "\n",
    "ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)', fontsize=12)\n",
    "ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)', fontsize=12)\n",
    "ax.set_title('PCA Scatter Plot (Feature-Based Clustering)', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(y=0, color='k', linestyle='--', alpha=0.3, linewidth=1)\n",
    "ax.axvline(x=0, color='k', linestyle='--', alpha=0.3, linewidth=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"04_pca_scatter_plot.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"✓ Saved to {OUTPUT_DIR / '04_pca_scatter_plot.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80224a3",
   "metadata": {},
   "source": [
    "## 8. Comparative Analysis and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5f6acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"COMPARATIVE ANALYSIS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Analysis 1: Correlation\n",
    "print(\"\\n1. CORRELATION ANALYSIS\")\n",
    "print(\"-\" * 80)\n",
    "max_corr_idx = np.unravel_index(np.argmax(np.triu(corr_matrix, k=1)), corr_matrix.shape)\n",
    "if max_corr_idx[0] != max_corr_idx[1]:\n",
    "    print(f\"Most correlated pair: {labels[max_corr_idx[0]]} <-> {labels[max_corr_idx[1]]}\")\n",
    "    print(f\"  Correlation coefficient: {corr_matrix[max_corr_idx[0], max_corr_idx[1]]:.3f}\")\n",
    "\n",
    "min_corr_idx = np.unravel_index(np.argmin(np.triu(corr_matrix) + np.diag([2]*n)), corr_matrix.shape)\n",
    "if min_corr_idx[0] != min_corr_idx[1]:\n",
    "    print(f\"\\nLeast correlated pair: {labels[min_corr_idx[0]]} <-> {labels[min_corr_idx[1]]}\")\n",
    "    print(f\"  Correlation coefficient: {corr_matrix[min_corr_idx[0], min_corr_idx[1]]:.3f}\")\n",
    "\n",
    "# Analysis 2: Derivative Correlation\n",
    "print(\"\\n2. DERIVATIVE CORRELATION ANALYSIS\")\n",
    "print(\"-\" * 80)\n",
    "max_deriv_idx = np.unravel_index(np.argmax(np.triu(deriv_corr_matrix, k=1)), deriv_corr_matrix.shape)\n",
    "if max_deriv_idx[0] != max_deriv_idx[1]:\n",
    "    print(f\"Most similar rate-of-change: {labels[max_deriv_idx[0]]} <-> {labels[max_deriv_idx[1]]}\")\n",
    "    print(f\"  Derivative correlation: {deriv_corr_matrix[max_deriv_idx[0], max_deriv_idx[1]]:.3f}\")\n",
    "\n",
    "# Analysis 3: DTW Distances\n",
    "print(\"\\n3. DTW DISTANCE ANALYSIS\")\n",
    "print(\"-\" * 80)\n",
    "min_dtw_idx = np.unravel_index(np.argmin(np.triu(dtw_matrix, k=1) + np.diag([np.inf]*n)), dtw_matrix.shape)\n",
    "if min_dtw_idx[0] != min_dtw_idx[1]:\n",
    "    print(f\"Most similar temporal patterns: {labels[min_dtw_idx[0]]} <-> {labels[min_dtw_idx[1]]}\")\n",
    "    print(f\"  DTW distance: {dtw_matrix[min_dtw_idx[0], min_dtw_idx[1]]:.3f}\")\n",
    "\n",
    "# Analysis 4: PCA Clustering\n",
    "print(\"\\n4. PCA FEATURE ANALYSIS\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Principal Component Loadings:\")\n",
    "for i in range(2):\n",
    "    print(f\"\\n  PC{i+1} ({pca.explained_variance_ratio_[i]:.1%} variance):\")\n",
    "    components = pca.components_[i]\n",
    "    top_idx = np.argsort(np.abs(components))[-3:][::-1]\n",
    "    for idx in top_idx:\n",
    "        print(f\"    {feature_names[idx]}: {components[idx]:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c56489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive summary table\n",
    "print(\"\\nDETAILED COMPARISON SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary_data = []\n",
    "for i, label in enumerate(labels):\n",
    "    summary_data.append({\n",
    "        'Series': label,\n",
    "        'Mean': f\"{aligned_series[label].mean():.3f}\",\n",
    "        'Std': f\"{aligned_series[label].std():.3f}\",\n",
    "        'Avg Corr': f\"{np.mean(corr_matrix[i]):.3f}\",\n",
    "        'Avg DTW': f\"{np.mean(dtw_matrix[i]):.3f}\",\n",
    "        'PC1': f\"{features_pca[i, 0]:.3f}\",\n",
    "        'PC2': f\"{features_pca[i, 1]:.3f}\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\n✓ All visualizations saved to: {OUTPUT_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
